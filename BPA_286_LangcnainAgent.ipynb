{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMX0013/Action_Agent/blob/wip/BPA_286_LangcnainAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "lh2OYGVlbotP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai streamlit python-dotenv\n",
        "!pip install langchainhub\n",
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot8fMoTBUeVu",
        "outputId": "180ebba7-0ba1-4dfe-dfe8-eb4dd834f077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.11)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.13.3)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.31.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.27)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.30)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.22)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (7.0.1)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.22.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.42)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.10/dist-packages (0.1.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0.20240218)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.30)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.13.3)\n",
            "Collecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (0.1.22)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.0.8 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Data Engineering -->"
      ],
      "metadata": {
        "id": "JxfZ1iP6bdPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ppYiOVNJbguh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def fetch_data_from_endpoint(URL, method):\n",
        "    \"\"\"\n",
        "    Author : Amudheswaran\n",
        "\n",
        "    Fetches data from an endpoint using the provided identifier.\n",
        "\n",
        "    params:\n",
        "       URL : The url string\n",
        "       method : The API endpoint\n",
        "    return: The dataframe obj.\n",
        "\n",
        "    Update (02/28/2024):Generalized proceesing both transcript and highlights endpoint\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"constructed URL : \", url)\n",
        "    api_key = 'a729aa23-26ea-415f-9bcb-5c340214ee0a'\n",
        "\n",
        "\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'x-api-key': api_key\n",
        "    }\n",
        "\n",
        "    # Construct GET request\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "\n",
        "      data = response.json()\n",
        "\n",
        "\n",
        "      match method:\n",
        "        case \"highlights\":\n",
        "          df = pd.DataFrame(data['data'])\n",
        "          df['category'] = df.apply(lambda row: row['category']['label'], axis=1)\n",
        "          output_file = \"highlights.xlsx\"\n",
        "\n",
        "        case \"transcript\":\n",
        "          # Split on fields\n",
        "          df = pd.DataFrame(data['data'])\n",
        "          output_file = \"transcript.xlsx\"\n",
        "\n",
        "\n",
        "      df.to_excel(output_file, index=False)\n",
        "      print(f\"Excel file '{output_file}' created successfully.\")\n",
        "\n",
        "      return df\n",
        "    else:\n",
        "      # Handle errors or unsuccessful responses\n",
        "      print(f\"Failed to fetch data: {response.status_code}\")\n",
        "      return None\n"
      ],
      "metadata": {
        "id": "ZUnDLIfebc8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Agent"
      ],
      "metadata": {
        "id": "CyQxVOsjbmI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# api_key=\"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\" # USe your own OpenAI API key\n",
        "identifier = \"65c3e1321f228f00140b0431\"\n",
        "method = \"highlights\"\n",
        "# Assuming the identifier is part of the URL\n",
        "url = f\"https://pasta.tldv.io/v1alpha1/meetings/{identifier}/{method}\"\n",
        "\n",
        "df = fetch_data_from_endpoint(url,method)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "MtsOmwvGvhup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65081e7-b114-4c4f-c38c-90a9187be7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constructed URL :  https://pasta.tldv.io/v1alpha1/meetings/65c3e1321f228f00140b0431/highlights\n",
            "Excel file 'highlights.xlsx' created successfully.\n",
            "                                                 text  startTime  source  \\\n",
            "0   Initial discussion around improving logging to...         20    auto   \n",
            "1   Agreement to keep monitoring logs for now and ...        623    auto   \n",
            "2   Discussion wraps up on log sheet and handoff t...        913    auto   \n",
            "3   Discussion on process called 'Calendar to Cale...        915    auto   \n",
            "4   Nabil brings up need for bug management system...       1335    auto   \n",
            "5   Group discusses using a shared sheet for now t...       1429    auto   \n",
            "6   Siddhant shares progress on building dynamic c...       1543    auto   \n",
            "7   Group discusses potential to scale calendars t...       1860    auto   \n",
            "8   Nabil shares proposed repository structure doc...       2097    auto   \n",
            "9   Amey suggests creating template folder with st...       2370    auto   \n",
            "10  Amey confirms that someone is not on the call....       2412    auto   \n",
            "11  The group discussed creating a template and fo...       2517    auto   \n",
            "12  @[Prajvala Sonawane](prajvala.sonawane@jerseys...       2579  manual   \n",
            "13  Discussion that the group will be some of the ...       3186    auto   \n",
            "14  Explanation of what information can be found w...       3313    auto   \n",
            "15  Explanation of additional summarization, actio...       3390    auto   \n",
            "16  Discussion that tldv currently generates trans...       3468    auto   \n",
            "17  Idea proposed to have a model listen for a key...       3546    auto   \n",
            "18  Potential first use case of linking meeting re...       4231    auto   \n",
            "19  Currently the linking has to be done manually,...       4340    auto   \n",
            "20  The plan is to extract only raw meeting data l...       4501    auto   \n",
            "21  The extracted raw data will then be analyzed u...       4514    auto   \n",
            "22  Discussion around defining the scope of what d...       4730    auto   \n",
            "23  The goal is to extract all possible data from ...       4740    auto   \n",
            "24  The task is to identify all possible data that...       4774    auto   \n",
            "25  The group discusses using Postman to explore t...       4819    auto   \n",
            "26  The group discusses using Postman to test out ...       4920    auto   \n",
            "27  The group notes that this is the first time th...       5011    auto   \n",
            "28  The group discusses making the Postman explora...       5045    auto   \n",
            "29  The group discusses checking off which API end...       5240    auto   \n",
            "30  The group discusses providing additional feedb...       5401    auto   \n",
            "31  @[Amudheswaran Sankaranarayanan](amudheswaran....       5702  manual   \n",
            "32  The group discusses adding annotations to meet...       5702    auto   \n",
            "33  The group discusses creative ways to combine s...       5821    auto   \n",
            "34  Produce spreadsheet with all APIs, Postman exp...       6116    auto   \n",
            "35  Spreadsheet to be used to store API payloads a...       6127    auto   \n",
            "36  Consider if regular SQL database suitable or i...       6180    auto   \n",
            "37  Need to programmatically transfer files receiv...       6728    auto   \n",
            "38  Will mimic the manual process of downloading a...       6782    auto   \n",
            "39  Nabil shares that the report is sent to a grou...       7232    auto   \n",
            "40  Nabil says he will add Prachi to the group tha...       7295    auto   \n",
            "41  Nabil clicks the 'Add Members' button to add P...       7327    auto   \n",
            "42  Nabil shares that the password for the service...       7352    auto   \n",
            "43  Nabil asks Prachi if she will now be able to l...       7362    auto   \n",
            "44  Nabil confirms this is a web-based application...       7416    auto   \n",
            "45  Prachi notes the downloaded file would need to...       7428    auto   \n",
            "46  Prachi agrees to start exploring the provided ...       7501    auto   \n",
            "47  Nabil invites Prachi to the 6PM meeting to int...       7535    auto   \n",
            "\n",
            "               category  \n",
            "0            Next Steps  \n",
            "1   General Information  \n",
            "2   General Information  \n",
            "3            Next Steps  \n",
            "4            Next Steps  \n",
            "5            Next Steps  \n",
            "6            Next Steps  \n",
            "7                 Ideas  \n",
            "8   General Information  \n",
            "9            Next Steps  \n",
            "10  General Information  \n",
            "11           Next Steps  \n",
            "12               No Tag  \n",
            "13           Next Steps  \n",
            "14  General Information  \n",
            "15           Next Steps  \n",
            "16       Open Questions  \n",
            "17           Next Steps  \n",
            "18  General Information  \n",
            "19  General Information  \n",
            "20  General Information  \n",
            "21           Next Steps  \n",
            "22  General Information  \n",
            "23  General Information  \n",
            "24           Next Steps  \n",
            "25  General Information  \n",
            "26  General Information  \n",
            "27  General Information  \n",
            "28  General Information  \n",
            "29           Next Steps  \n",
            "30           Next Steps  \n",
            "31               No Tag  \n",
            "32                Ideas  \n",
            "33                Ideas  \n",
            "34  General Information  \n",
            "35  General Information  \n",
            "36       Open Questions  \n",
            "37           Next Steps  \n",
            "38                Ideas  \n",
            "39  General Information  \n",
            "40  General Information  \n",
            "41  General Information  \n",
            "42  General Information  \n",
            "43  General Information  \n",
            "44           Next Steps  \n",
            "45           Next Steps  \n",
            "46           Next Steps  \n",
            "47           Next Steps  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASIC Implementation"
      ],
      "metadata": {
        "id": "PNBASiZw1T5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "class CustomAgent:\n",
        "    def __init__(self, api_key):\n",
        "        self.llm = OpenAI(api_key=api_key)\n",
        "\n",
        "    def process_transcript(self, df):\n",
        "        # Aggregate the conversation text from the DataFrame\n",
        "        conversation_text = ' '.join(df['text'].tolist())\n",
        "\n",
        "        # Define the prompt to extract action items\n",
        "        prompt = f\"Identify the assigner, assignee, task_Application, task_decription, task_identifiers mentioned based on unique Members in each  and return as labelled JSON payload like :\\n{conversation_text}\"\n",
        "        print(prompt)\n",
        "\n",
        "        # Generate the response from the LLM using the invoke method\n",
        "\n",
        "        response = self.llm.invoke(input=prompt, model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "        # Extract and return the text response\n",
        "        print(response)\n",
        "\n",
        "# Usage example\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8SeZHKoUqWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create your .env file and manually write your API key in there"
      ],
      "metadata": {
        "id": "gUtZhIEtMFX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "env_file_path = os.path.join(os.getcwd(), \".env\")  # Get current working directory\n",
        "with open(env_file_path, \"a+\") as file:  # Open in append mode to create if absent\n",
        "    header_exists = file.readline()  # Check for existing header\n",
        "    if not header_exists:\n",
        "      file.write(\"# Example .env file\\n\")  # Add a header if the file is newly created\n",
        "    file.write(f\"API_KEY={API_key}\\n\")  # Ensure API_KEY is written even if a header exists\n",
        "\n",
        "\n",
        "# api_key = os,enviro"
      ],
      "metadata": {
        "id": "ug45x5LELDAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "identifier = \"65e8cb3b7a3386001396f704\"\n",
        "method = \"highlights\"\n",
        "# Assuming the identifier is part of the URL\n",
        "url = f\"https://pasta.tldv.io/v1alpha1/meetings/{identifier}/{method}\"\n",
        "\n",
        "df = fetch_data_from_endpoint(url,method)\n",
        "df\n",
        "\n",
        "\n",
        "agent = CustomAgent(api_key=api_key)\n",
        "# Process the transcript and get action items\n",
        "action_items = agent.process_transcript(df)\n",
        "print(action_items)"
      ],
      "metadata": {
        "id": "acdT_G2YnQUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48ebeeb-8242-4771-84c8-562f06ea3329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constructed URL :  https://pasta.tldv.io/v1alpha1/meetings/65e8cb3b7a3386001396f704/highlights\n",
            "Excel file 'highlights.xlsx' created successfully.\n",
            "Identify the assigner, assignee, task_Application, task_decription, task_identifiers mentioned based on unique Members in each  and return as labelled JSON payload like :\n",
            "Nabil asks Amey to open a spreadsheet that should have been processed the previous day to check if it has been updated properly. Nabil asks Amey to check the dates on attachments to Google Drive spreadsheets to ensure they are being updated correctly by the process. Nabil notes they need proof that programs are running and delivering expected outputs by checking spreadsheet update dates. Amey mentions code they were working on to fetch last update dates that could be added to process logging. Amey discusses options for using the fetch function directly in sheets or as part of process scripts. Nabil proposes keeping an inventory of all impacted sheets and comparing to sheets updated by each process hourly. Lakshaye introduces himself. Nabil assigns him to help with Postman training and adoption. Ajinkya asks Nabil if Uber's system has any restrictive access that could be causing the authentication failure. Prachi mentions she was able to connect with a different account. Prachi will reach out to the person who set up the SFTP server to add Ajinkya's account. TASK is to get Ajinkya's account access to the server. Nabil assigns Lakshaye's first task to meet with Saket to understand the push to Sheets API issue using Postman. BRAINSTORMING how to resolve it. Anagha provides an update on their work to connect to Sheets via the API and issues they faced. Next step is to continue troubleshooting. Govardhan found the Handshake documentation and is able to extract data without filters. Next step is adding the filtering functionality. Nabil shows Mayank the quick filters on a Jira board and explains the need to extract the filter names programmatically and put them in a spreadsheet. He mentions using Selenium for web scraping. Nabil and Mayank discuss the syntax to access the quick filter configuration page and agree it's better to generate the link programmatically. Nabil wants Mayank to work with Muneeb on this task. Nabil suggests setting up a meeting with Muneeb to coordinate on the Jira quick filters extraction work. Nabil mentions the Chrome extensions project as something Mayank could help with since it involves backend work and data access. Nabil continues discussing potential areas for Mayank to help with given his experience. Mayank suggests moving data from Associates to a database. Nabil responds that their data analytics team handles database selection and setup. Nabil assigns Mayank the selenium training documentation to review as part of onboarding. Mayank asks if he can start on March 1st. Nabil agrees but asks him to complete 21 hours of work that week to justify the earlier start date. Preeti informs Nabil that Rahul has likely received a job offer and is looking for apartments in the Bay Area. Preeti and Nabil discuss finding someone to take over Rahul's responsibilities related to Slack integration.\n",
            "\n",
            "\n",
            "{\n",
            "    \"Nabil\": {\n",
            "        \"assigner\": true,\n",
            "        \"assignee\": false,\n",
            "        \"task_application\": false,\n",
            "        \"task_description\": \"Nabil asks Amey to open a spreadsheet that should have been processed the previous day to check if it has been updated properly. Nabil asks Amey to check the dates on attachments to Google Drive spreadsheets to ensure they are being updated correctly by the process. Nabil notes they need proof that programs are running and delivering expected outputs by checking spreadsheet update dates.\",\n",
            "        \"task_identifiers\": [\"spreadsheet\", \"process\", \"Google Drive\", \"proof\", \"programs\", \"expected outputs\", \"spreadsheet update dates\"]\n",
            "    },\n",
            "    \"Amey\": {\n",
            "        \"assigner\": false,\n",
            "        \"assignee\": true,\n",
            "        \"task_application\": false,\n",
            "        \"task_description\": \"Amey mentions code they were working on to fetch last update dates that could be added to process logging. Amey discusses options for using the fetch function directly in sheets or as part of process scripts.\",\n",
            "        \"task_identifiers\": [\"code\", \"fetch\", \"last update dates\", \"process logging\", \"fetch function\", \"sheets\", \"process scripts\"]\n",
            "    },\n",
            "    \"Lakshaye\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_openai import OpenAI\n",
        "import pandas as pd\n",
        "from langchain import hub"
      ],
      "metadata": {
        "id": "-WRk3kUbS2WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced model with prompt templates."
      ],
      "metadata": {
        "id": "mmQBkoLm1fB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# obj = hub.pull(\"hwchase17/react\")\n",
        "class ReActAgent:\n",
        "    def __init__(self, api_key):\n",
        "        self.llm = OpenAI(api_key=api_key)\n",
        "        # Define the prompt for classification and extraction\n",
        "        self.classification_prompt = \"Determine if the following text is a Task mentioning an Assigner, Assignee, and a Task to be done. Return 'true' if it does, otherwise 'false'.\"\n",
        "        self.extraction_prompt = \"Extract the Assigner, Assignee, Task ID detail, and Task Description from the following text and create a string which represents a JSON object containing list of structured data extracted:\"\n",
        "\n",
        "        template = \"\"\"\n",
        "        Use the following format:\n",
        "\n",
        "        Question: the input question you must answer\n",
        "        Thought: you should always think about what to do\n",
        "        Action: Process the {}\n",
        "        Observation: the result of the action\n",
        "        ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "        Thought: I now know the final answer\n",
        "        Final Answer: the final answer to the original input question\n",
        "\n",
        "        Begin!\n",
        "\n",
        "        Question: {input}\n",
        "        Thought: Does this sentence contain whatever the {input} asked for\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        # No external tools required for this example\n",
        "        tools = []  # If you have any tools, add them here\n",
        "\n",
        "        # prompt =  hub.pull(\"hwchase17/react\")\n",
        "\n",
        "        # Construct the ReAct agent without using hub.pull\n",
        "        self.agent = create_react_agent(self.llm, tools, prompt=prompt)\n",
        "        # this react agent is a problem. It will only work when {'tool_names', 'agent_scratchpad', 'tools'} exists\n",
        "\n",
        "\n",
        "    def process_transcript(self, df):\n",
        "        tasks = []  # To store extracted tasks\n",
        "\n",
        "        # Create an AgentExecutor instance\n",
        "        agent_executor = AgentExecutor(agent=self.agent, tools=[], verbose=True)\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            input_text = row['text']\n",
        "\n",
        "            # Step 1: Classify the row\n",
        "            classification_context = {\"input\": f\"{self.classification_prompt}\", \"text\": f'{input_text}'}\n",
        "            print(classification_context)\n",
        "            classification_result = agent_executor.invoke(classification_context)\n",
        "\n",
        "            if classification_result.get('output', '').strip().lower() == 'true':\n",
        "                # Step 2: Extract task details\n",
        "                extraction_context = {\"input\": f\"{self.extraction_prompt}\\n\\nText: '{input_text}'\"}\n",
        "                extraction_result = agent_executor.invoke(extraction_context)\n",
        "\n",
        "                # Assuming the extraction_result['output'] is structured text that needs to be parsed\n",
        "                # Implement a parsing logic based on the output format\n",
        "                # For simplicity, let's assume it returns a dict-like string that can be evaluated safely\n",
        "                task_info = eval(extraction_result['output'])  # Consider using a safer parsing method\n",
        "\n",
        "                task = Task(**task_info)\n",
        "                tasks.append(task)\n",
        "\n",
        "        # Processed tasks available here\n",
        "        for task in tasks:\n",
        "            print(task)\n",
        "\n"
      ],
      "metadata": {
        "id": "ujs7GiVVRyU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **USAGE**"
      ],
      "metadata": {
        "id": "yPz7R_Qe1bu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Task data class\n",
        "@dataclass\n",
        "class Task:\n",
        "    Assignee: str\n",
        "    Assigner: str\n",
        "    Task_ID_detail: str\n",
        "    Task_Description: str\n",
        "\n",
        "# Example usage:\n",
        "identifier = \"65e8cb3b7a3386001396f704\"\n",
        "method = \"highlights\"\n",
        "# Assuming the identifier is part of the URL\n",
        "url = f\"https://pasta.tldv.io/v1alpha1/meetings/{identifier}/{method}\"\n",
        "\n",
        "df = fetch_data_from_endpoint(url,method)\n",
        "# Assuming 'api_key' is your OpenAI API key and 'df' is your pandas DataFrame\n",
        "API_key=\"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\" # USe your own OpenAI API key\n",
        "agent = ReActAgent(api_key=API_key)\n",
        "agent.process_transcript(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "vwI_VYPlS55j",
        "outputId": "cf3784be-cda6-4eab-a71e-a66b76a92321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constructed URL :  https://pasta.tldv.io/v1alpha1/meetings/65e8cb3b7a3386001396f704/highlights\n",
            "Excel file 'highlights.xlsx' created successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Prompt missing required variables: {'tool_names', 'agent_scratchpad', 'tools'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d621a2b36852>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Assuming 'api_key' is your OpenAI API key and 'df' is your pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mAPI_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"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\"\u001b[0m \u001b[0;31m# USe your own OpenAI API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReActAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAPI_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-873187da32d7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Construct the ReAct agent without using hub.pull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_react_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# this react agent is a problem. It lioi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/react/agent.py\u001b[0m in \u001b[0;36mcreate_react_agent\u001b[0;34m(llm, tools, prompt, output_parser, tools_renderer)\u001b[0m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmissing_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt missing required variables: {missing_vars}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     prompt = prompt.partial(\n",
            "\u001b[0;31mValueError\u001b[0m: Prompt missing required variables: {'tool_names', 'agent_scratchpad', 'tools'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "Create a slack bot or a means to drop these in there\n",
        "https://www.youtube.com/watch?v=Luujq0t0J7A&ab_channel=DataTechInfo\n",
        "\n"
      ],
      "metadata": {
        "id": "BKhx-rfMCSbt"
      }
    }
  ]
}